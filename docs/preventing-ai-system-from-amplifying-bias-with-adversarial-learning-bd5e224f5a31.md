# é˜²æ­¢äººå·¥æ™ºèƒ½ç³»ç»Ÿé€šè¿‡å¯¹æŠ—æ€§å­¦ä¹ æ”¾å¤§åè§

> åŸæ–‡ï¼š<https://medium.com/hackernoon/preventing-ai-system-from-amplifying-bias-with-adversarial-learning-bd5e224f5a31>

## å¯¹æŠ—å­¦ä¹ ä¸­åè§çš„ä»‹ç»ã€åŸå› åŠè§£å†³æ–¹æ³•

![](img/0771f6192054ef5a05f1832be09ca88d.png)

**è¯´åˆ°æ”¾å¤§åå·®ï¼Œæˆ‘ä»¬é¦–å…ˆæƒ³åˆ°çš„é—®é¢˜æ˜¯ï¼ŒAI ç³»ç»Ÿæ˜¯å¦‚ä½•æ”¾å¤§åå·®çš„ï¼Ÿ**

åˆ¤åˆ«æˆ–ç”Ÿæˆæ¨¡å‹æ˜¯åå·®æ”¾å¤§çš„åŸå› ã€‚å› ä¸ºåˆ¤åˆ«æ¨¡å‹æ›´åƒæ˜¯â€œé»‘ç®±â€,åªå­¦ä¹ å›ç­”ç‰¹å®šçš„è®­ç»ƒæ•°æ®é›†ã€‚

**ä¸‹ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œä»€ä¹ˆæ˜¯åˆ¤åˆ«æ¨¡å‹ï¼Œå®ƒæ˜¯å¦‚ä½•å¯¼è‡´åè§çš„ï¼Ÿ**

åˆ¤åˆ«æ¨¡å‹ä¹Ÿç§°ä¸ºæ¡ä»¶æ¨¡å‹ï¼Œå®ƒè¯•å›¾ä»…æ ¹æ®è§‚å¯Ÿåˆ°çš„æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ï¼ŒåŒæ—¶å­¦ä¹ å¦‚ä½•ä»ç»™å®šçš„ç»Ÿè®¡æ•°æ®ä¸­è¿›è¡Œåˆ†ç±»ã€‚åˆ¤åˆ«æ¨¡å‹ï¼Œå¦‚ç¥ç»ç½‘ç»œã€é€»è¾‘å›å½’ã€SVMã€æ¡ä»¶éšæœºåœºã€‚

**é‚£ä¹ˆï¼Œäººå·¥æ™ºèƒ½ç®—æ³•æ˜¯å¦‚ä½•æ”¾å¤§åå·®çš„ï¼Ÿ**

**ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä»¥å•è¯åµŒå…¥åå·®ä¸ºä¾‹ï¼Œå®ƒæ˜¯æ”¾å¤§åå·®çš„æ¥æºã€‚**

æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„åŠ›é‡ä¸ä»…é¢„ç¤ºç€å·¨å¤§çš„æŠ€æœ¯è¿›æ­¥ï¼Œä¹Ÿå¸¦æ¥äº†ç¤¾ä¼šå±å®³çš„é£é™©ã€‚æµè¡Œçš„å•è¯åµŒå…¥ç®—æ³•è¡¨ç°å‡ºåˆ»æ¿åè§ï¼Œä¾‹å¦‚æ€§åˆ«åè§ã€‚è¿™äº›ç®—æ³•åœ¨æœºå™¨å­¦ä¹ ç³»ç»Ÿä¸­çš„å¹¿æ³›ä½¿ç”¨ï¼Œä»è‡ªåŠ¨ç¿»è¯‘æœåŠ¡åˆ°ç®€å†æ‰«æä»ªï¼Œå¯ä»¥åœ¨é‡è¦çš„èƒŒæ™¯ä¸‹æ”¾å¤§åˆ»æ¿å°è±¡ã€‚å°½ç®¡å·²ç»å¼€å‘äº†æµ‹é‡è¿™äº›åå·®å’Œæ”¹å˜å•è¯åµŒå…¥ä»¥å‡è½»å®ƒä»¬çš„æœ‰åå·®è¡¨ç¤ºçš„æ–¹æ³•ï¼Œä½†æ˜¯ç¼ºä¹å¯¹å•è¯åµŒå…¥åå·®å¦‚ä½•ä¾èµ–äºè®­ç»ƒæ•°æ®çš„ç†è§£ã€‚ç»™å®šåœ¨è¯­æ–™åº“ä¸Šè®­ç»ƒçš„å•è¯åµŒå…¥ï¼Œç‰¹å®šåå·®åº¦é‡æ–¹æ³•è¯†åˆ«å¯¹è¯­æ–™åº“çš„æ‰°åŠ¨å°†å¦‚ä½•å½±å“æ‰€å¾—åµŒå…¥çš„åå·®ã€‚è¿™å¯ä»¥ç”¨æ¥è¿½æº¯å•è¯åµŒå…¥åå·®çš„æ¥æºï¼Œè¿½æº¯åˆ°åŸå§‹è®­ç»ƒæ–‡æ¡£ã€‚

![](img/6b7d9065f0436da4f214de619a8997eb.png)

Fig shows how certain cultural context of information learned by Athe I system leads to bias

**å®ƒä»¬å¦‚ä½•å½±å“ï¼Ÿ**

å¦‚æœæ‰§è¡Œåˆ†ç±»ä»»åŠ¡ï¼Œä»–ä»¬å¯èƒ½ä¼šæ¥å—æœ€å¤§åŒ–åˆ†ç±»å‡†ç¡®æ€§çš„åŸ¹è®­ã€‚è¿™æ„å‘³ç€æ¨¡å‹å°†åˆ©ç”¨ä»»ä½•æœ‰åŠ©äºæé«˜æ•°æ®é›†å‡†ç¡®æ€§çš„ä¿¡æ¯ï¼Œå°¤å…¶æ˜¯æ•°æ®ä¸­å­˜åœ¨çš„ä»»ä½•åå·®ã€‚

ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ä¸€ä¸ªå®æ—¶è‡ªåŠ¨åŒ–æ‹›è˜å¹³å°çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œå®ƒæ˜¯å¦‚ä½•å½±å“å€™é€‰äººè€Œä¸è¿›å…¥å·¥ä½œçš„ã€‚

**â€œäºšé©¬é€ŠæŠ›å¼ƒäº†äººå·¥æ™ºèƒ½æ‹›è˜å·¥å…·ï¼Œè¯¥å·¥å…·åçˆ±ä»äº‹æŠ€æœ¯å·¥ä½œçš„ç”·æ€§ã€‚â€**

**æ€ä¹ˆå‘ç”Ÿçš„ï¼Ÿ**

è¯¥å…¬å¸çš„å®éªŒæ€§æ‹›è˜å·¥å…·ä½¿ç”¨äººå·¥æ™ºèƒ½ç»™æ±‚èŒè€…æ‰“åˆ†ï¼Œä»ä¸€é¢—æ˜Ÿåˆ°äº”é¢—æ˜Ÿä¸ç­‰ï¼Œå°±åƒè´­ç‰©è€…åœ¨äºšé©¬é€Šä¸Šç»™äº§å“æ‰“åˆ†ä¸€æ ·ã€‚â€œä»–ä»¬çœŸçš„å¸Œæœ›å®ƒæ˜¯ä¸€ä¸ªå¼•æ“ï¼Œæˆ‘ä¼šç»™ä½  100 ä»½ç®€å†ï¼Œå®ƒä¼šåˆ†å‡ºå‰äº”åï¼Œæˆ‘ä»¬ä¼šé›‡ç”¨ä»–ä»¬ã€‚â€

![](img/bb0588cac6d4b97fa18fa971fd7d3940.png)

Amazon recruitment System bias against women

ä½†åˆ° 2015 å¹´ï¼Œè¯¥å…¬å¸æ„è¯†åˆ°å…¶æ–°ç³»ç»Ÿå¹¶æ²¡æœ‰ä»¥æ€§åˆ«ä¸­ç«‹çš„æ–¹å¼å¯¹è½¯ä»¶å¼€å‘å·¥ä½œå’Œå…¶ä»–æŠ€æœ¯èŒä½çš„å€™é€‰äººè¿›è¡Œè¯„çº§ã€‚è¿™æ˜¯å› ä¸ºäºšé©¬é€Šæ‹›è˜å¹³å°ä¸­çš„â€œ**äººå·¥æ™ºèƒ½æ¨¡å‹â€æ˜¯é€šè¿‡è§‚å¯Ÿ 10 å¹´é—´æäº¤ç»™è¯¥å…¬å¸çš„ç®€å†æ¨¡å¼æ¥è®­ç»ƒç”³è¯·äººçš„ã€‚å¤§å¤šæ•°æ¥è‡ªç”·æ€§ï¼Œè¿™å¯¼è‡´äº†ç”·æ€§åœ¨æ•´ä¸ªæ‹›è˜å¹³å°ä¸Šçš„ä¸»å¯¼åœ°ä½ã€‚**

**å¦‚ä½•å¤„ç†äººå·¥æ™ºèƒ½(NLP)ç³»ç»Ÿä¸­æ€§åˆ«ä¸­ç«‹æ–¹å¼çš„åè§ï¼Ÿ**

å•è¯åµŒå…¥æ¨¡å‹å·²ç»æˆä¸ºå¹¿æ³›çš„è‡ªç„¶è¯­è¨€å¤„ç†(NLP)åº”ç”¨ä¸­çš„åŸºæœ¬ç»„ä»¶ã€‚ç„¶è€Œï¼Œåœ¨äººç±»ç”Ÿæˆçš„è¯­æ–™åº“ä¸Šè®­ç»ƒçš„åµŒå…¥å·²ç»è¢«è¯æ˜ç»§æ‰¿äº†åæ˜ ç¤¾ä¼šç»“æ„çš„å¼ºçƒˆçš„æ€§åˆ«åˆ»æ¿å°è±¡ã€‚

åµŒå…¥æ˜¯å°†ç¦»æ•£å˜é‡(ä¾‹å¦‚å•è¯ã€åœ°åŒºã€URL)æŠ•å½±åˆ°å¤šç»´å®å€¼ç©ºé—´çš„å¼ºå¤§æœºåˆ¶ã€‚å·²ç»å¼€å‘äº†å‡ ç§å¼ºæœ‰åŠ›çš„æ–¹æ³•æ¥å­¦ä¹ åµŒå…¥ã€‚ä¸€ä¸ªä¾‹å­æ˜¯è·³æ ¼ç®—æ³•ã€‚åœ¨è¯¥ç®—æ³•ä¸­ï¼Œå‘¨å›´çš„ä¸Šä¸‹æ–‡ç”¨äºé¢„æµ‹å•è¯çš„å­˜åœ¨ã€‚ä¸å¹¸çš„æ˜¯ï¼Œè®¸å¤šçœŸå®ä¸–ç•Œçš„æ–‡æœ¬æ•°æ®éƒ½æœ‰ä¸€ä¸ªå¾®å¦™çš„åå·®ï¼Œæœºå™¨å­¦ä¹ ç®—æ³•ä¼šéšå¼åœ°å°†å®ƒåŒ…å«åœ¨ä»è¯¥æ•°æ®åˆ›å»ºçš„åµŒå…¥ä¸­ã€‚è¿™ç§åå·®å¯ä»¥é€šè¿‡ä½¿ç”¨å­¦ä¹ åˆ°çš„åµŒå…¥æ¥æ‰§è¡Œå•è¯ç±»æ¯”ä»»åŠ¡æ¥è¯´æ˜

![](img/1867a42ee6876ce59b742494526a76ff.png)

ç°åœ¨ï¼Œè®©æˆ‘ä»¬è®¨è®ºä¸€ä¸‹å‡è½»åè§çš„æœ€æœ‰æ•ˆçš„å¯¹æŠ—æ€§å­¦ä¹ ã€‚

# å¯¹æŠ—æ€§å­¦ä¹ å¦‚ä½•æœ‰åŠ©äºå‡è½»åè§ï¼Ÿ

å¯¹æŠ—æ³•æ¶ˆé™¤äº†åµŒå…¥ä¸­çš„ä¸€äº›åè§ï¼Œå®ƒåŸºäºè¿™æ ·ä¸€ç§æ€æƒ³ï¼Œå³é‚£äº›åµŒå…¥æ—¨åœ¨ç”¨äºé¢„æµ‹åŸºäºè¾“å…¥ğ‘‹çš„ä¸€äº›ç»“æœğ‘Œï¼Œä½†æ˜¯åœ¨å…¬å¹³çš„ä¸–ç•Œä¸­ï¼Œè¯¥ç»“æœåº”è¯¥ä¸ä¸€äº›å—ä¿æŠ¤çš„å˜é‡ğ‘.å®Œå…¨æ— å…³å¦‚æœæ˜¯è¿™æ ·çš„è¯ï¼Œé‚£ä¹ˆäº†è§£ğ‘Œå¹¶ä¸ä¼šå¸®åŠ©ä½ æ›´å¥½åœ°é¢„æµ‹ğ‘ã€‚è¿™ä¸ªåŸç†å¯ä»¥ç›´æ¥è½¬åŒ–ä¸ºå¦‚ä¸‹å›¾æ‰€ç¤ºçš„ä¸¤ä¸ªä¸²è”ç½‘ç»œã€‚ç¬¬ä¸€æ¬¡å°è¯•ä½¿ç”¨ğ‘‹ä½œä¸ºè¾“å…¥æ¥é¢„æµ‹ğ‘Œã€‚ç¬¬äºŒæ¬¡å°è¯•ä½¿ç”¨ğ‘Œçš„é¢„æµ‹å€¼æ¥é¢„æµ‹ğ‘.å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œ

![](img/8c3371846b4120e84dbad6e99f29deb0.png)

The architecture of Adversarial network

ç„¶è€Œï¼Œç®€å•åœ°è®­ç»ƒåŸºäºâˆ‡ğ‘Šğ¿1 çš„ w ä¸­çš„æƒé‡å’ŒåŸºäºâˆ‡ğ‘ˆğ¿2 çš„ğ‘ˆä¸­çš„æƒé‡å®é™…ä¸Šä¸ä¼šå®ç°æ— åçš„æ¨¡å‹ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œä½ éœ€è¦åœ¨ğ‘Š's æ›´æ–°å‡½æ•°ä¸­åŠ å…¥è¿™æ ·ä¸€ä¸ªæ¦‚å¿µï¼Œå³ğ‘ˆåœ¨é¢„æµ‹ğ‘.æ—¶åº”è¯¥ä¸ä¼šæ¯”æœºä¼šæ›´å¥½ä½ å¯ä»¥å®ç°è¿™ä¸€ç‚¹çš„æ–¹å¼ç±»ä¼¼äºç”Ÿæˆæ•Œå¯¹ç½‘ç»œ(GANs) ( [Goodfellow ç­‰äºº 2014](http://papers.nips.cc/paper/5423-generative-adversarial-nets) )è®­ç»ƒå…¶ç”Ÿæˆå™¨çš„æ–¹å¼ã€‚

é™¤äº†âˆ‡ğ‘Šğ¿1ï¼Œä½ è¿˜æŠŠå¯¹âˆ‡ğ‘Šğ¿2 çš„å¦å®šçº³å…¥äº†ğ‘Š's çš„æ›´æ–°å‡½æ•°ã€‚ç„¶è€Œï¼Œæœ‰å¯èƒ½âˆ‡ğ‘Šğ¿1 æ­£åœ¨æ”¹å˜ğ‘Šï¼Œé€šè¿‡ä½¿ç”¨ä½ è¯•å›¾ä¿æŠ¤çš„æœ‰åè§çš„ä¿¡æ¯æ¥æé«˜å‡†ç¡®æ€§ã€‚ä¸ºäº†é¿å…è¿™ä¸€ç‚¹ï¼Œä½ è¿˜åŠ å…¥äº†ä¸€ä¸ªæœ¯è¯­ï¼Œé€šè¿‡å°†âˆ‡ğ‘Šğ¿1 çš„æˆåˆ†æŠ•å°„åˆ°âˆ‡ğ‘Šğ¿2.ï¼Œæ¥ç§»é™¤å®ƒä¸€æ—¦å°†è¿™ä¸¤ä¸ªæœ¯è¯­åˆå¹¶ï¼Œğ‘Šçš„æ›´æ–°å‡½æ•°å°±å˜æˆäº†:

âˆ‡ğ‘Šğ¿1âˆ’ğ‘ğ‘Ÿğ‘œğ‘—(âˆ‡ğ‘Šğ¿2)âˆ‡ğ‘Šğ¿1âˆ’âˆ‡ğ‘Šğ¿2

å¯¹å¦‚ä½•å°†æ•Œå¯¹ç½‘ç»œåˆå¹¶åˆ°æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„æè¿°æ˜¯éå¸¸é€šç”¨çš„ï¼Œå› ä¸ºè¯¥æŠ€æœ¯é€šå¸¸é€‚ç”¨äºä»»ä½•ç±»å‹çš„ç³»ç»Ÿï¼Œè¿™äº›ç³»ç»Ÿå¯ä»¥æ ¹æ®é¢„æµ‹ğ‘Œçš„è¾“å…¥ğ‘‹æ¥æè¿°ï¼Œä½†æ˜¯å¯èƒ½åŒ…å«å…³äºå—ä¿æŠ¤å˜é‡ğ‘.çš„ä¿¡æ¯åªè¦ä½ èƒ½æ„é€ ç›¸å…³çš„æ›´æ–°å‡½æ•°ï¼Œä½ å°±èƒ½åº”ç”¨è¿™ç§æŠ€æœ¯ã€‚ç„¶è€Œï¼Œè¿™å¹¶ä¸èƒ½å‘Šè¯‰ä½ å¤ªå¤šå…³äºğ‘‹ã€ğ‘Œå’Œğ‘.çš„æœ¬è´¨åœ¨å•è¯ç±»æ¯”ä»»åŠ¡ä¸­ï¼Œğ‘‹ =ğµ+ğ¶âˆ’ğ´å’Œğ‘Œ=ğ·.ä¸è¿‡ï¼Œè¦å¼„æ¸…æ¥šğ‘åº”è¯¥æ˜¯ä»€ä¹ˆæ ·å­ï¼Œè¦ç¨å¾®å¤æ‚ä¸€ç‚¹ã€‚ä¸ºæ­¤ï¼Œè¯·å‚è€ƒ Bulokbasi ç­‰äººçš„ä¸€ç¯‡è®ºæ–‡ã€‚è‰¾å°”ã€‚ä»–ä»¬å¼€å‘äº†ä¸€ç§æ— ç›‘ç£çš„æ–¹æ³•æ¥ä»å•è¯åµŒå…¥ä¸­å»é™¤æ€§åˆ«è¯­ä¹‰ã€‚

**ç°åœ¨ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨ä½¿ç”¨å¯¹æŠ—æ€§å­¦ä¹ æ¥å‡è½»åè§çš„å®ç°éƒ¨åˆ†**

ç¬¬ä¸€æ­¥æ˜¯é€‰æ‹©ä¸è¯•å›¾æ¶ˆé™¤çš„åè§ç±»å‹ç›¸å…³çš„è¯å¯¹ã€‚å°±æ€§åˆ«è€Œè¨€ï¼Œé€‰æ‹©åƒâ€œç”·äººâ€ã€â€œå¥³äººâ€ã€â€œç”·å­©â€ã€â€œå¥³å­©â€è¿™æ ·çš„è¯å¯¹ï¼Œå®ƒä»¬åœ¨è¯­ä¹‰ä¸Šçš„å”¯ä¸€åŒºåˆ«å°±æ˜¯æ€§åˆ«ã€‚è¿™äº›è¯å¯¹å¯ä»¥è®¡ç®—å®ƒä»¬çš„åµŒå…¥ä¹‹é—´çš„å·®å¼‚ï¼Œä»¥åœ¨åµŒå…¥çš„è¯­ä¹‰ç©ºé—´ä¸­äº§ç”Ÿå¤§è‡´å¹³è¡Œäºæ€§åˆ«è¯­ä¹‰çš„å‘é‡ã€‚å¯¹è¿™äº›å‘é‡æ‰§è¡Œä¸»æˆåˆ†åˆ†æ(PCA ),ç„¶åç»™å‡ºç”±æ‰€æä¾›çš„æ€§åˆ«åŒ–å•è¯å¯¹å®šä¹‰çš„æ€§åˆ«è¯­ä¹‰çš„ä¸»è¦æˆåˆ†ã€‚å› æ­¤ï¼Œè®©æˆ‘ä»¬å®šä¹‰ç”¨äºæ‰§è¡ŒåµŒå…¥çš„ä¸»è¦ç»„ä»¶çš„å‡½æ•°ï¼Œ

```
def find_gender_direction(embed,
                          indices):
  """Finds and returns a 'gender direction'."""
  pairs = [
      ("woman", "man"),
      ("her", "his"),
      ("she", "he"),
      ("aunt", "uncle"),
      ("niece", "nephew"),
      ("daughters", "sons"),
      ("mother", "father"),
      ("daughter", "son"),
      ("granddaughter", "grandson"),
      ("girl", "boy"),
      ("stepdaughter", "stepson"),
      ("mom", "dad"),
  ]
  m = []
  for wf, wm in pairs:
    m.append(embed[indices[wf]] - embed[indices[wm]])
  m = np.array(m)# the next three lines are just a PCA.
  m = np.cov(np.array(m).T)
  evals, evecs = np.linalg.eig(m)
  return _np_normalize(np.real(evecs[:, np.argmax(evals)]))# Using the embeddings, find the gender vector.
gender_direction = find_gender_direction(embed, indices)
print "gender direction: %s" % str(gender_direction.flatten())
```

ä¸€æ—¦å®Œæˆäº†åµŒå…¥å·®å¼‚çš„ç¬¬ä¸€ä¸ªä¸»æˆåˆ†ï¼Œå¼€å§‹æŠŠå•è¯çš„åµŒå…¥æŠ•å°„åˆ°å®ƒä¸Šé¢ã€‚è¿™ä¸ªé¢„æµ‹å¯ä»¥ä½œä¸ºå¯¹æ‰‹è¯•å›¾æ ¹æ®ğ‘Œ.çš„é¢„æµ‹å€¼é¢„æµ‹çš„å—ä¿æŠ¤å˜é‡ğ‘ç°åœ¨è®©æˆ‘ä»¬æ¥çœ‹çœ‹å¯¹æ€§åˆ«ç»´åº¦æœ‰æœ€å¤§è´Ÿé¢å½±å“çš„å•è¯ã€‚

```
words = set()
for a in analogies:
  words.update(a)df = pd.DataFrame(data={"word": list(words)})
df["gender_score"] = df["word"].map(
    lambda w: client.word_vec(w).dot(gender_direction))
df.sort_values(by="gender_score", inplace=True)
print df.head(10)
```

ç°åœ¨è®©æˆ‘ä»¬æ¥çœ‹çœ‹é‚£äº›åœ¨æ€§åˆ«ç»´åº¦ä¸Šæœ‰æœ€å¤§*æ­£é¢*æŠ•å°„çš„å•è¯ã€‚

```
df.sort_values(by="gender_score", inplace=True, ascending=False)
print df.head(10)
```

# è®­ç»ƒæ¨¡å‹

è®­ç»ƒæ•Œå¯¹ç½‘ç»œæ˜¯å›°éš¾çš„ã€‚ä»–ä»¬å¾ˆæ•æ„Ÿï¼Œå¦‚æœæ¥è§¦çš„æ–¹å¼ä¸å¯¹ï¼Œä»–ä»¬å¾ˆå¿«å°±ä¼šçˆ†å‘ã€‚å¿…é¡»éå¸¸å°å¿ƒåœ°ä»¥è¶³å¤Ÿæ…¢çš„é€Ÿåº¦è®­ç»ƒä¸¤ä¸ªæ¨¡å‹ï¼Œä»¥ä¾¿æ¨¡å‹ä¸­çš„å‚æ•°ä¸ä¼šå‘æ•£ã€‚å®é™…ä¸Šï¼Œè¿™é€šå¸¸éœ€è¦æ˜¾è‘—é™ä½åˆ†ç±»å™¨å’Œå¯¹æ‰‹çš„æ­¥é•¿ã€‚å°†å¯¹æ‰‹çš„å‚æ•°åˆå§‹åŒ–ä¸ºæå°ä¹Ÿå¯èƒ½æ˜¯æœ‰ç›Šçš„ï¼Œä»¥ç¡®ä¿åˆ†ç±»å™¨ä¸ä¼šè¿‡åº¦é€‚åº”ç‰¹å®šçš„(æ¬¡ä¼˜)å¯¹æ‰‹(è¿™ç§è¿‡åº¦é€‚åº”ä¼šå¾ˆå¿«å¯¼è‡´å‘æ•£ï¼).ä¹Ÿæœ‰å¯èƒ½çš„æ˜¯ï¼Œå¦‚æœåˆ†ç±»å™¨å¤ªæ“…é•¿äºå¯¹å¯¹æ‰‹éšè—å—ä¿æŠ¤çš„å˜é‡ï¼Œé‚£ä¹ˆå¯¹æ‰‹å°†å¼ºåŠ å‘æ•£çš„æ›´æ–°ä»¥åŠªåŠ›æé«˜å…¶æ€§èƒ½ã€‚è§£å†³æ–¹æ³•æœ‰æ—¶æ˜¯å®é™…ä¸Šå¢åŠ å¯¹æ‰‹çš„å­¦ä¹ é€Ÿç‡ä»¥é˜²æ­¢å‘æ•£(è¿™åœ¨å¤§å¤šæ•°å­¦ä¹ ç³»ç»Ÿä¸­å‡ ä¹æ˜¯é—»æ‰€æœªé—»çš„)ã€‚åœ¨æˆ‘çš„ [**GitHub**](https://github.com/rashmimarganiatgithub/preventing_bias_adversarial) **ï¼Œ**ä¸­å¯ä»¥æ‰¾åˆ°ç›¸åŒçš„å•è¯åµŒå…¥çš„å»åç½®æ¨¡å‹ï¼Œè¯·æŸ¥çœ‹å®ƒä»¥é‡ç°å®éªŒã€‚ä¸‹é¢æ˜¯è®­ç»ƒæ¨¡å‹çš„ä»£ç ã€‚

```
class AdversarialEmbeddingModel(object):
  """A model for doing adversarial training of embedding models."""def __init__(self, client,
               data_p, embed_dim, projection,
               projection_dims, pred):
    """Creates a new AdversarialEmbeddingModel.Args:
      client: The (possibly biased) embeddings.
      data_p: Placeholder for the data.
      embed_dim: Number of dimensions used in the embeddings.
      projection: The space onto which we are "projecting".
      projection_dims: Number of dimensions of the projection.
      pred: Prediction layer.
    """
    # load the analogy vectors as well as the embeddings
    self.client = client
    self.data_p = data_p
    self.embed_dim = embed_dim
    self.projection = projection
    self.projection_dims = projection_dims
    self.pred = preddef nearest_neighbors(self, sess, in_arr,
                        k):
    """Finds the nearest neighbors to a vector.Args:
      sess: Session to use.
      in_arr: Vector to find nearest neighbors to.
      k: Number of nearest neighbors to return
    Returns:
      List of up to k pairs of (word, score).
    """
    v = sess.run(self.pred, feed_dict={self.data_p: in_arr})
    return self.client.similar_by_vector(v.flatten().astype(float), topn=k)def write_to_file(self, sess, f):
    """Writes a model to disk."""
    np.savetxt(f, sess.run(self.projection))def read_from_file(self, sess, f):
    """Reads a model from disk."""
    loaded_projection = np.loadtxt(f).reshape(
        [self.embed_dim, self.projection_dims])
    sess.run(self.projection.assign(loaded_projection))def fit(self,
          sess,
          data,
          data_p,
          labels,
          labels_p,
          protect,
          protect_p,
          gender_direction,
          pred_learning_rate,
          protect_learning_rate,
          protect_loss_weight,
          num_steps,
          batch_size,
          debug_interval=1000):
    """Trains a model.Args:
      sess: Session.
      data: Features for the training data.
      data_p: Placeholder for the features for the training data.
      labels: Labels for the training data.
      labels_p: Placeholder for the labels for the training data.
      protect: Protected variables.
      protect_p: Placeholder for the protected variables.
      gender_direction: The vector from find_gender_direction().
      pred_learning_rate: Learning rate for predicting labels.
      protect_learning_rate: Learning rate for protecting variables.
      protect_loss_weight: The constant 'alpha' found in
          debias_word_embeddings.ipynb.
      num_steps: Number of training steps.
      batch_size: Number of training examples in each step.
      debug_interval: Frequency at which to log performance metrics during
          training.
    """
    feed_dict = {
        data_p: data,
        labels_p: labels,
        protect_p: protect,
    }
    # define the prediction loss
    pred_loss = tf.losses.mean_squared_error(labels_p, self.pred)# compute the prediction of the protected variable.
    # The "trainable"/"not trainable" designations are for the predictor. The
    # adversary explicitly specifies its own list of weights to train.
    protect_weights = tf.get_variable(
        "protect_weights", [self.embed_dim, 1], trainable=False)
    protect_pred = tf.matmul(self.pred, protect_weights)
    protect_loss = tf.losses.mean_squared_error(protect_p, protect_pred)pred_opt = tf.train.AdamOptimizer(pred_learning_rate)
    protect_opt = tf.train.AdamOptimizer(protect_learning_rate)protect_grad = {v: g for (g, v) in pred_opt.compute_gradients(protect_loss)}
    pred_grad = []# applies the gradient expression found in the document linked
    # at the top of this file.
    for (g, v) in pred_opt.compute_gradients(pred_loss):
      unit_protect = tf_normalize(protect_grad[v])
      # the two lines below can be commented out to train without debiasing
      g -= tf.reduce_sum(g * unit_protect) * unit_protect
      g -= protect_loss_weight * protect_grad[v]
      pred_grad.append((g, v))
      pred_min = pred_opt.apply_gradients(pred_grad)# compute the loss of the protected variable prediction.
    protect_min = protect_opt.minimize(protect_loss, var_list=[protect_weights])sess.run(tf.global_variables_initializer())
    sess.run(tf.local_variables_initializer())
    step = 0
    while step < num_steps:
      # pick samples at random without replacement as a minibatch
      ids = np.random.choice(len(data), batch_size, False)
      data_s, labels_s, protect_s = data[ids], labels[ids], protect[ids]
      sgd_feed_dict = {
          data_p: data_s,
          labels_p: labels_s,
          protect_p: protect_s,
      }if not step % debug_interval:
        metrics = [pred_loss, protect_loss, self.projection]
        metrics_o = sess.run(metrics, feed_dict=feed_dict)
        pred_loss_o, protect_loss_o, proj_o = metrics_o
        # log stats every so often: number of steps that have passed,
        # prediction loss, adversary loss
        print("step: %d; pred_loss_o: %f; protect_loss_o: %f" % (step,
                     pred_loss_o, protect_loss_o))
        for i in range(proj_o.shape[1]):
          print("proj_o: %f; dot(proj_o, gender_direction): %f)" %
                       (np.linalg.norm(proj_o[:, i]),
                       np.dot(proj_o[:, i].flatten(), gender_direction)))
      sess.run([pred_min, protect_min], feed_dict=sgd_feed_dict)
      step += 1

def filter_analogies(analogies,
                     index_map):
  filtered_analogies = []
  for analogy in analogies:
    if filter(index_map.has_key, analogy) != analogy:
      print "at least one word missing for analogy: %s" % analogy
    else:
      filtered_analogies.append(map(index_map.get, analogy))
  return filtered_analogiesdef make_data(
    analogies, embed,
    gender_direction):
  """Preps the training data.Args:
    analogies: a list of analogies
    embed: the embedding matrix from load_vectors
    gender_direction: the gender direction from find_gender_directionReturns:
    Three numpy arrays corresponding respectively to the input, output, and
    protected variables.
  """
  data = []
  labels = []
  protect = []
  for analogy in analogies:
    # the input is just the word embeddings of the first three words
    data.append(embed[analogy[:3]])
    # the output is just the word embeddings of the last word
    labels.append(embed[analogy[3]])
    # the protected variable is the gender component of the output embedding.
    # the extra pair of [] is so that the array has the right shape after
    # it is converted to a numpy array.
    protect.append([np.dot(embed[analogy[3]], gender_direction)])
  # Convert all three to numpy arrays, and return them.
  return tuple(map(np.array, (data, labels, protect))
```

å¯¹æŠ—æ–¹æ³•æœ‰åŠ©äºå‡å°‘å•è¯åµŒå…¥ä¸­çš„åå·®ï¼Œå¹¶ä¸”å¯ä»¥å¾ˆå¥½åœ°æ¨å¹¿åˆ°å…¶ä»–é¢†åŸŸå’Œä»»åŠ¡ã€‚é€šè¿‡è¯•å›¾å¯¹å¯¹æ‰‹éšè—å—ä¿æŠ¤å˜é‡ï¼Œæœºå™¨å­¦ä¹ ç³»ç»Ÿå¯ä»¥å‡å°‘ç³»ç»Ÿä¸­éšå«çš„å…³äºå—ä¿æŠ¤å˜é‡çš„æœ‰åè§çš„ä¿¡æ¯é‡ã€‚é™¤äº†ç‰¹å®šçš„æ–¹æ³•ä¹‹å¤–ï¼Œåœ¨è¿™ä¸ªä¸»é¢˜ä¸Šè¿˜æœ‰è®¸å¤šå˜ä½“ï¼Œå¯ä»¥ç”¨æ¥å®ç°ä¸åŒç¨‹åº¦å’Œç±»å‹çš„å»åç½®ã€‚

å¸Œæœ›ä½ å–œæ¬¢é˜…è¯»è¿™ä¸ªæ•…äº‹ï¼Œå¹¶å‘ç°å®ƒæ˜¯æœ‰å¸®åŠ©çš„ã€‚è°¢è°¢ä½ ã€‚